{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97549b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.image as img\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e38cfc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "980342e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #setting parameter for image data agumentationto the training data\n",
    "    train_datagen=ImageDataGenerator(rescale=1./225,shear_range=0.2,zoom_range=0.2,horizontal_flip=True)\n",
    "    #image data agumenatation to test dataset\n",
    "    test_datagen=ImageDataGenerator(rescale=1./225)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a75fdccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2080 images belonging to 3 classes.\n",
      "Found 496 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "#performing data agumentation to train the dataset\n",
    "x_train=train_datagen.flow_from_directory(directory=r'C:\\Users\\namit\\Downloads\\Assignment\\data\\train',target_size=(64,64),batch_size=32,class_mode='categorical')\n",
    "#performing agumentation to test the dataset\n",
    "x_test=test_datagen.flow_from_directory(directory=r'C:\\Users\\namit\\Downloads\\Assignment\\data/test',target_size=(64,64),batch_size=32,class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed00b79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03ead242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D\n",
    "import tensorflow.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cb66a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding model layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b6a1529",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "\n",
    "pretrained_model= tf.keras.applications.VGG16(include_top=False,\n",
    "                   input_shape=(160,160,3),\n",
    "                   pooling='avg',classes=3,\n",
    "                   weights='imagenet')\n",
    "\n",
    "for layer in pretrained_model.layers:\n",
    "        layer.trainable=False\n",
    "\n",
    "model.add(pretrained_model)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b16e522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0db4c9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "opt = Adam(lr=0.001)\n",
    "model.compile(optimizer=opt, loss=tensorflow.keras.losses.categorical_crossentropy, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af2515d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Functional)           (None, 512)               14714688  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 14,978,883\n",
      "Trainable params: 264,195\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9eac0fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e990175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-d2c10514fe5b>:1: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/32\n",
      "65/65 [==============================] - 66s 1s/step - loss: 0.7212 - accuracy: 0.6798 - val_loss: 0.5855 - val_accuracy: 0.7480\n",
      "Epoch 2/32\n",
      "65/65 [==============================] - 65s 999ms/step - loss: 0.5843 - accuracy: 0.7673 - val_loss: 0.6378 - val_accuracy: 0.7319\n",
      "Epoch 3/32\n",
      "65/65 [==============================] - 50s 764ms/step - loss: 0.5631 - accuracy: 0.7615 - val_loss: 0.6016 - val_accuracy: 0.7601\n",
      "Epoch 4/32\n",
      "65/65 [==============================] - 17s 260ms/step - loss: 0.5010 - accuracy: 0.7894 - val_loss: 0.6248 - val_accuracy: 0.7339\n",
      "Epoch 5/32\n",
      "65/65 [==============================] - 19s 288ms/step - loss: 0.4959 - accuracy: 0.7952 - val_loss: 0.5768 - val_accuracy: 0.7500\n",
      "Epoch 6/32\n",
      "65/65 [==============================] - 19s 295ms/step - loss: 0.4634 - accuracy: 0.8091 - val_loss: 0.5774 - val_accuracy: 0.7500\n",
      "Epoch 7/32\n",
      "65/65 [==============================] - 20s 304ms/step - loss: 0.4521 - accuracy: 0.8087 - val_loss: 0.6354 - val_accuracy: 0.7339\n",
      "Epoch 8/32\n",
      "65/65 [==============================] - 20s 309ms/step - loss: 0.4468 - accuracy: 0.8168 - val_loss: 0.5202 - val_accuracy: 0.7863\n",
      "Epoch 9/32\n",
      "65/65 [==============================] - 20s 303ms/step - loss: 0.4285 - accuracy: 0.8288 - val_loss: 0.5827 - val_accuracy: 0.7681\n",
      "Epoch 10/32\n",
      "65/65 [==============================] - 20s 308ms/step - loss: 0.4265 - accuracy: 0.8346 - val_loss: 0.5767 - val_accuracy: 0.7500\n",
      "Epoch 11/32\n",
      "65/65 [==============================] - 20s 305ms/step - loss: 0.3950 - accuracy: 0.8351 - val_loss: 0.5355 - val_accuracy: 0.7722\n",
      "Epoch 12/32\n",
      "65/65 [==============================] - 20s 310ms/step - loss: 0.3911 - accuracy: 0.8438 - val_loss: 0.5960 - val_accuracy: 0.7440\n",
      "Epoch 13/32\n",
      "65/65 [==============================] - 22s 337ms/step - loss: 0.3610 - accuracy: 0.8577 - val_loss: 0.6326 - val_accuracy: 0.7480\n",
      "Epoch 14/32\n",
      "65/65 [==============================] - 22s 335ms/step - loss: 0.3629 - accuracy: 0.8514 - val_loss: 0.6684 - val_accuracy: 0.7298\n",
      "Epoch 15/32\n",
      "65/65 [==============================] - 23s 350ms/step - loss: 0.3506 - accuracy: 0.8558 - val_loss: 0.5417 - val_accuracy: 0.7641\n",
      "Epoch 16/32\n",
      "65/65 [==============================] - 22s 332ms/step - loss: 0.3136 - accuracy: 0.8769 - val_loss: 0.5482 - val_accuracy: 0.7782\n",
      "Epoch 17/32\n",
      "65/65 [==============================] - 22s 331ms/step - loss: 0.3114 - accuracy: 0.8774 - val_loss: 0.6932 - val_accuracy: 0.7480\n",
      "Epoch 18/32\n",
      "65/65 [==============================] - 21s 327ms/step - loss: 0.3098 - accuracy: 0.8736 - val_loss: 0.5671 - val_accuracy: 0.7762\n",
      "Epoch 19/32\n",
      "65/65 [==============================] - 22s 345ms/step - loss: 0.3008 - accuracy: 0.8793 - val_loss: 0.5500 - val_accuracy: 0.7702\n",
      "Epoch 20/32\n",
      "65/65 [==============================] - 21s 322ms/step - loss: 0.3011 - accuracy: 0.8798 - val_loss: 0.5711 - val_accuracy: 0.7702\n",
      "Epoch 21/32\n",
      "65/65 [==============================] - 22s 336ms/step - loss: 0.2677 - accuracy: 0.9062 - val_loss: 0.6196 - val_accuracy: 0.7581\n",
      "Epoch 22/32\n",
      "65/65 [==============================] - 21s 321ms/step - loss: 0.2822 - accuracy: 0.8904 - val_loss: 0.5510 - val_accuracy: 0.7702\n",
      "Epoch 23/32\n",
      "65/65 [==============================] - 21s 323ms/step - loss: 0.2481 - accuracy: 0.9048 - val_loss: 0.5362 - val_accuracy: 0.7782\n",
      "Epoch 24/32\n",
      "65/65 [==============================] - 22s 341ms/step - loss: 0.2422 - accuracy: 0.9115 - val_loss: 0.6546 - val_accuracy: 0.7681\n",
      "Epoch 25/32\n",
      "65/65 [==============================] - 22s 332ms/step - loss: 0.2246 - accuracy: 0.9245 - val_loss: 0.6759 - val_accuracy: 0.7601\n",
      "Epoch 26/32\n",
      "65/65 [==============================] - 22s 341ms/step - loss: 0.2237 - accuracy: 0.9178 - val_loss: 0.7677 - val_accuracy: 0.7379\n",
      "Epoch 27/32\n",
      "65/65 [==============================] - 22s 335ms/step - loss: 0.2138 - accuracy: 0.9216 - val_loss: 0.7719 - val_accuracy: 0.7540\n",
      "Epoch 28/32\n",
      "65/65 [==============================] - 22s 345ms/step - loss: 0.2122 - accuracy: 0.9255 - val_loss: 0.6200 - val_accuracy: 0.7843\n",
      "Epoch 29/32\n",
      "65/65 [==============================] - 21s 330ms/step - loss: 0.2277 - accuracy: 0.9130 - val_loss: 0.6094 - val_accuracy: 0.7661\n",
      "Epoch 30/32\n",
      "65/65 [==============================] - 22s 338ms/step - loss: 0.1965 - accuracy: 0.9303 - val_loss: 0.6815 - val_accuracy: 0.7782\n",
      "Epoch 31/32\n",
      "65/65 [==============================] - 21s 320ms/step - loss: 0.1959 - accuracy: 0.9317 - val_loss: 0.6683 - val_accuracy: 0.7702\n",
      "Epoch 32/32\n",
      "65/65 [==============================] - 21s 325ms/step - loss: 0.1948 - accuracy: 0.9260 - val_loss: 0.7224 - val_accuracy: 0.7601\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21cf094e970>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(generator=x_train,steps_per_epoch = len(x_train), epochs=32, validation_data=x_test,validation_steps = len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0e98cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Nosh.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d39885ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecad1fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(fn):\n",
    "    image = Image.open(fn)\n",
    "    return np.asarray(image.resize((160,160)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37327207",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_predict(test_image):\n",
    "    result = model.predict(np.asarray([read_image(test_image)]))\n",
    "\n",
    "    itemindex = np.where(result==np.max(result))\n",
    "    prediction = itemindex[1][0]\n",
    "    print(\"probability: \"+str(np.max(result)*100) + \"%\\nPredicted class : \", prediction)\n",
    "\n",
    "    image = img.imread(test_image)\n",
    "    plt.imshow(image)\n",
    "    plt.title(prediction)\n",
    "    index=['sitting ','sleeping','standing']\n",
    "    result = str(index[prediction])\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd8ff2af",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-d40df37bfe24>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:\\Users\\namit\\Desktop\\Assignment_nosh\\test/Image_4292.JPG\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-25-0a38a5e5ae5a>\u001b[0m in \u001b[0;36mtest_predict\u001b[1;34m(test_image)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtest_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mread_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mitemindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitemindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-6d0a31ae3d47>\u001b[0m in \u001b[0;36mread_image\u001b[1;34m(fn)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m160\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m160\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59dfa12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 160, 160, 3) for input Tensor(\"vgg16_input_3:0\", shape=(None, 160, 160, 3), dtype=float32), but it was called on an input with incompatible shape (None, 64, 64, 3).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 160, 160, 3) for input Tensor(\"input_4_2:0\", shape=(None, 160, 160, 3), dtype=float32), but it was called on an input with incompatible shape (None, 64, 64, 3).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "model=load_model(r'C:\\Users\\namit\\Downloads\\Assignment\\Nosh.N5')\n",
    "img=image.load_img(r\"C:\\Users\\namit\\Desktop\\Assignment_nosh\\test/Image_1007.JPG\",target_size=(64,64))\n",
    "x=image.img_to_array(img)\n",
    "x=np.expand_dims(x,axis=0)\n",
    "pred = model.predict(x)\n",
    "y_pred=np.argmax(pred)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ed52aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 160, 160, 3) for input Tensor(\"vgg16_input_4:0\", shape=(None, 160, 160, 3), dtype=float32), but it was called on an input with incompatible shape (None, 64, 64, 3).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 160, 160, 3) for input Tensor(\"input_4_3:0\", shape=(None, 160, 160, 3), dtype=float32), but it was called on an input with incompatible shape (None, 64, 64, 3).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "model=load_model(r'C:\\Users\\namit\\Downloads\\Assignment\\Nosh.N5')\n",
    "img=image.load_img(r\"C:\\Users\\namit\\Desktop\\Assignment_nosh\\test/Image_994.JPG\",target_size=(64,64))\n",
    "x=image.img_to_array(img)\n",
    "x=np.expand_dims(x,axis=0)\n",
    "pred = model.predict(x)\n",
    "y_pred=np.argmax(pred)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ba546a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 160, 160, 3) for input Tensor(\"vgg16_input_5:0\", shape=(None, 160, 160, 3), dtype=float32), but it was called on an input with incompatible shape (None, 64, 64, 3).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 160, 160, 3) for input Tensor(\"input_4_4:0\", shape=(None, 160, 160, 3), dtype=float32), but it was called on an input with incompatible shape (None, 64, 64, 3).\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000021C8C7F9EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "model=load_model(r'C:\\Users\\namit\\Downloads\\Assignment\\Nosh.N5')\n",
    "img=image.load_img(r\"C:\\Users\\namit\\Desktop\\Assignment_nosh\\test/Image_4292.JPG\",target_size=(64,64))\n",
    "x=image.img_to_array(img)\n",
    "x=np.expand_dims(x,axis=0)\n",
    "pred = model.predict(x)\n",
    "y_pred=np.argmax(pred)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b5389dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 160, 160, 3) for input Tensor(\"vgg16_input_6:0\", shape=(None, 160, 160, 3), dtype=float32), but it was called on an input with incompatible shape (None, 64, 64, 3).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 160, 160, 3) for input Tensor(\"input_4_5:0\", shape=(None, 160, 160, 3), dtype=float32), but it was called on an input with incompatible shape (None, 64, 64, 3).\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000021C8B694700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "model=load_model(r'C:\\Users\\namit\\Downloads\\Assignment\\Nosh.N5')\n",
    "img=image.load_img(r\"C:\\Users\\namit\\Desktop\\Assignment_nosh\\test/Image_1030.JPG\",target_size=(64,64))\n",
    "x=image.img_to_array(img)\n",
    "x=np.expand_dims(x,axis=0)\n",
    "pred = model.predict(x)\n",
    "y_pred=np.argmax(pred)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f543f7e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
